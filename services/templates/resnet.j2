# -*- coding: utf-8 -*-
# ============================================================================
# Template: resnet.j2
# Purpose : ResNet (bottleneck or basic) for image classification (Keras 3 safe)
# ============================================================================

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
# {% raw %}{{CUSTOM_BLOCK:imports_extra}}{% endraw %}

# ---------------------------
# 1) 하이퍼파라미터 (spec.dims에서 주입)
# ---------------------------
input_shape      = {{ dims.input_shape     | default([224, 224, 3]) | pylit }}
num_classes      = {{ dims.num_classes     | default(1000) }}
stem_filters     = {{ dims.stem_filters    | default(64) }}
stage_blocks     = {{ dims.stage_blocks    | default([3,4,6,3]) | pylit }}     # per stage
stage_filters    = {{ dims.stage_filters   | default([64,128,256,512]) | pylit }}
bottleneck       = {{ dims.bottleneck      | default(True) | pylit }}         # True: bottleneck(1x1-3x3-1x1), False: basic(3x3-3x3)
use_se           = {{ dims.use_se          | default(False) | pylit }}        # Squeeze-and-Excitation on/off
se_reduction     = {{ dims.se_reduction    | default(16) }}
stochastic_depth = {{ dims.stochastic_depth_p | default(0.0) }}               # 0.0 ~ 0.5 권장
dropout_rate     = {{ dims.dropout         | default(0.0) }}

learning_rate  = {{ learning_rate | default(1e-3) }}
optimizer_name = {{ optimizer_name | default("adam") | tojson }}
metrics        = {{ metrics        | default(['accuracy']) | tojson }}
loss_fn        = {{ loss           | default("sparse_categorical_crossentropy") | tojson }}

# ---------------------------
# 2) 유틸 레이어/블록
# ---------------------------
class StochasticDepth(layers.Layer):
    """Per-sample stochastic depth (a.k.a. DropPath)."""
    def __init__(self, drop_prob: float, **kwargs):
        super().__init__(**kwargs)
        self.drop_prob = float(drop_prob)

    def call(self, x, training=None):
        if (not training) or self.drop_prob <= 0.0:
            return x
        keep_prob = 1.0 - self.drop_prob
        # Uniform mask per-sample, broadcast over HWC
        batch = tf.shape(x)[0]
        random_tensor = keep_prob + tf.random.uniform([batch, 1, 1, 1], 0, 1)
        binary_tensor = tf.floor(random_tensor)
        # rescale to keep expectation
        return (x / keep_prob) * binary_tensor

class SEBlock(layers.Layer):
    """Squeeze-and-Excitation."""
    def __init__(self, channels: int, reduction: int = 16, **kwargs):
        super().__init__(**kwargs)
        self.channels = int(channels)
        self.reduction = int(max(1, reduction))
        self.gap = layers.GlobalAveragePooling2D()
        self.fc1 = layers.Dense(max(1, self.channels // self.reduction), activation="relu")
        self.fc2 = layers.Dense(self.channels, activation="sigmoid")
        self.reshape = layers.Reshape((1,1,self.channels))

    def call(self, x):
        s = self.gap(x)
        s = self.fc1(s)
        s = self.fc2(s)
        s = self.reshape(s)
        return x * s

def conv_bn_relu(x, filters, k, s=1, name=None):
    x = layers.Conv2D(filters, k, strides=s, padding="same", use_bias=False, name=None if name is None else f"{name}_conv")(x)
    x = layers.BatchNormalization(name=None if name is None else f"{name}_bn")(x)
    x = layers.ReLU(name=None if name is None else f"{name}_relu")(x)
    return x

class BasicBlock(layers.Layer):
    expansion = 1
    def __init__(self, filters, stride=1, use_se=False, se_reduction=16, drop_prob=0.0, **kwargs):
        super().__init__(**kwargs)
        self.filters = int(filters)
        self.stride = int(stride)
        self.use_se = bool(use_se)
        self.drop = StochasticDepth(drop_prob)
        self.conv1 = layers.Conv2D(self.filters, 3, strides=self.stride, padding="same", use_bias=False)
        self.bn1   = layers.BatchNormalization()
        self.relu1 = layers.ReLU()
        self.conv2 = layers.Conv2D(self.filters, 3, strides=1, padding="same", use_bias=False)
        self.bn2   = layers.BatchNormalization()
        if self.use_se:
            self.se = SEBlock(self.filters, se_reduction)
        else:
            self.se = None
        self.proj  = None  # projection for skip if needed

    def build(self, input_shape):
        in_ch = int(input_shape[-1])
        out_ch = self.filters * self.expansion
        if (self.stride != 1) or (in_ch != out_ch):
            self.proj = keras.Sequential([
                layers.Conv2D(out_ch, 1, strides=self.stride, padding="same", use_bias=False),
                layers.BatchNormalization()
            ])

    def call(self, x, training=None):
        shortcut = x
        x = self.conv1(x)
        x = self.bn1(x, training=training)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.bn2(x, training=training)
        if self.se is not None:
            x = self.se(x)
        if self.proj is not None:
            shortcut = self.proj(shortcut, training=training)
        x = layers.Add()([x, shortcut])
        x = self.drop(x, training=training)  # stochastic depth after add
        return layers.ReLU()(x)

class BottleneckBlock(layers.Layer):
    expansion = 4
    def __init__(self, filters, stride=1, use_se=False, se_reduction=16, drop_prob=0.0, **kwargs):
        super().__init__(**kwargs)
        self.filters = int(filters)
        self.stride = int(stride)
        self.use_se = bool(use_se)
        self.drop = StochasticDepth(drop_prob)
        width = self.filters
        self.conv1 = layers.Conv2D(width, 1, strides=1, padding="same", use_bias=False)
        self.bn1   = layers.BatchNormalization()
        self.conv2 = layers.Conv2D(width, 3, strides=self.stride, padding="same", use_bias=False)
        self.bn2   = layers.BatchNormalization()
        self.conv3 = layers.Conv2D(self.filters * self.expansion, 1, strides=1, padding="same", use_bias=False)
        self.bn3   = layers.BatchNormalization()
        self.relu  = layers.ReLU()
        if self.use_se:
            self.se = SEBlock(self.filters * self.expansion, se_reduction)
        else:
            self.se = None
        self.proj  = None

    def build(self, input_shape):
        in_ch = int(input_shape[-1])
        out_ch = self.filters * self.expansion
        if (self.stride != 1) or (in_ch != out_ch):
            self.proj = keras.Sequential([
                layers.Conv2D(out_ch, 1, strides=self.stride, padding="same", use_bias=False),
                layers.BatchNormalization()
            ])

    def call(self, x, training=None):
        shortcut = x
        x = self.conv1(x); x = self.bn1(x, training=training); x = self.relu(x)
        x = self.conv2(x); x = self.bn2(x, training=training); x = self.relu(x)
        x = self.conv3(x); x = self.bn3(x, training=training)
        if self.se is not None:
            x = self.se(x)
        if self.proj is not None:
            shortcut = self.proj(shortcut, training=training)
        x = layers.Add()([x, shortcut])
        x = self.drop(x, training=training)
        return self.relu(x)

def make_stage(x, filters, blocks, block_cls, base_prob, end_prob, start_idx, name_prefix):
    """
    stage 내 블록 수만큼 선형적으로 drop_prob를 증가시키며 쌓는다.
    """
    for i in range(blocks):
        # 선형 스케줄: start_idx 기반 글로벌 인덱스가 있으면 좋지만,
        # 여기서는 stage 내에서 0..blocks-1로만 간단히 스케일
        t = i / max(1, (blocks - 1))
        drop_p = base_prob + (end_prob - base_prob) * t
        stride = 2 if i == 0 and name_prefix != "stage0" else 1
        x = block_cls(filters, stride=stride, use_se=use_se, se_reduction=se_reduction,
                      drop_prob=drop_p, name=f"{name_prefix}_b{i}")(x)
    return x

# ---------------------------
# 3) 모델 구성
# ---------------------------
def build_model():
    inputs = keras.Input(shape=tuple(input_shape), name="image")

    # Stem
    x = layers.Conv2D(stem_filters, 7, strides=2, padding="same", use_bias=False, name="stem_conv")(inputs)
    x = layers.BatchNormalization(name="stem_bn")(x)
    x = layers.ReLU(name="stem_relu")(x)
    x = layers.MaxPooling2D(pool_size=3, strides=2, padding="same", name="stem_pool")(x)

    # 블록 타입 선택
    block_cls = BottleneckBlock if bottleneck else BasicBlock
    # 전체 stochastic depth를 stage별 균등 분배 (간단화)
    total_stages = len(stage_blocks)
    base_p = 0.0
    end_p  = float(stochastic_depth)

    # Stages
    # {% raw %}{{CUSTOM_BLOCK:stages}}{% endraw %}
    for si, (filters, blocks) in enumerate(zip(stage_filters, stage_blocks)):
        x = make_stage(x, filters, int(blocks), block_cls, base_p, end_p, si, name_prefix=f"stage{si}")

    # {% raw %}{{CUSTOM_BLOCK:head}}{% endraw %}
    # Head
    x = layers.GlobalAveragePooling2D(name="gap")(x)
    if dropout_rate and dropout_rate > 0.0:
        x = layers.Dropout(dropout_rate, name="head_drop")(x)

    if int(num_classes) == 1:
        outputs = layers.Dense(1, name="reg_head")(x)
    else:
        outputs = layers.Dense(int(num_classes), activation="softmax", name="cls_head")(x)

    model = keras.Model(inputs, outputs, name="resnet")

    # Optimizer
    opt = optimizer_name
    if isinstance(optimizer_name, str):
        low = optimizer_name.lower()
        if low == "adam":
            opt = keras.optimizers.Adam(learning_rate=learning_rate)
        elif low == "sgd":
            opt = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
        elif low == "rmsprop":
            opt = keras.optimizers.RMSprop(learning_rate=learning_rate)
        else:
            opt = keras.optimizers.get(optimizer_name)

    # {% raw %}{{CUSTOM_BLOCK:compile_override}}{% endraw %}
    model.compile(optimizer=opt, loss=loss_fn, metrics=metrics)
    return model
