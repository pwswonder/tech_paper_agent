# services/summarizer.py
# ------------------------------------------------------------
# ëª©ì : ê¸°ìˆ  ë…¼ë¬¸ ìš”ì•½/QA ì—ì´ì „íŠ¸ í’ˆì§ˆ í–¥ìƒ
# í•µì‹¬: ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ì™€ QAìš© í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ë¦¬ + ê·œì¹™ ê°•í™”
# ------------------------------------------------------------

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import AzureChatOpenAI
from langchain_core.runnables import RunnableLambda
from typing import List
import os
from dotenv import load_dotenv
from langsmith import traceable

# 1) í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env ê²½ë¡œê°€ ë£¨íŠ¸ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ í•„ìš” ì‹œ ì§€ì •)
load_dotenv()

# 2) LLM ì¸ìŠ¤í„´ìŠ¤ ë¶„ë¦¬
# - ìš”ì•½ì€ ì‚¬ì‹¤ì„±/ì¼ê´€ì„± ì¤‘ìš” â†’ temperature ë‚®ê²Œ, í† í° ë„‰ë„‰íˆ
summary_llm = AzureChatOpenAI(
    # azure_deployment=os.getenv("AOAI_DEPLOY_GPT40"),
    # openai_api_version="2024-02-01",
    azure_deployment=os.getenv("AOAI_DEPLOY_GPT41"),
    openai_api_version="2024-10-21",
    api_key=os.getenv("AOAI_API_KEY"),
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    temperature=0.1,
    # max_tokens=900,  # ìš”ì•½ ë¶„ëŸ‰ ì œì–´
)

# - QAëŠ” ì‘ë‹µ ë‹¤ì–‘ì„± ì•½ê°„ í—ˆìš©
qa_llm = AzureChatOpenAI(
    azure_deployment=os.getenv("AOAI_DEPLOY_GPT41"),
    openai_api_version="2024-10-21",
    api_key=os.getenv("AOAI_API_KEY"),
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    temperature=0.3,
    # max_tokens=700,  # QA ë‹µë³€ ê¸¸ì´ ì œì–´
)

# 3) ìš”ì•½ ì „ìš© System í”„ë¡¬í”„íŠ¸ (ê·œì¹™ ê°•ë ¥)
SUMMARY_SYSTEM = """\
ë‹¹ì‹ ì€ ê¸°ìˆ  ë…¼ë¬¸ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ì—„ê²©íˆ ë”°ë¥´ì„¸ìš”.

[ëª©í‘œ]
- ì œê³µëœ í…ìŠ¤íŠ¸(ë…¼ë¬¸ ì²­í¬)ë§Œì„ ê·¼ê±°ë¡œ, **ì‚¬ì‹¤ì— ê·¼ê±°í•œ** í•œêµ­ì–´ ìš”ì•½ì„ ì‘ì„±í•©ë‹ˆë‹¤.
- ì•„ë˜ ì„¹ì…˜ í—¤ë”ë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ê³ , ê° ì„¹ì…˜ì€ 1~4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°íˆ ì‘ì„±í•©ë‹ˆë‹¤.

[ì„¹ì…˜]
1) ì—°êµ¬ì£¼ì œ
2) í•µì‹¬ ê¸°ì—¬ (ë¶ˆë¦¿ 2~3ê°œ)
3) ë°©ë²•(ëª¨ë¸/ì•Œê³ ë¦¬ì¦˜/êµ¬ì¡°)
4) ë°ì´í„°ì…‹/ì„¤ì • (ë°ì´í„°, ì „ì²˜ë¦¬, ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°)
5) ì‹¤í—˜ê²°ê³¼ (ì •ëŸ‰ ì°¨ì´/ì§€í‘œ/ë¹„êµ ëŒ€ìƒ í¬í•¨)

[ì‚¬ì‹¤ì„±/ì¸ìš©]
- ìˆ«ì/ëª¨ë¸ëª…/ë°ì´í„°ì…‹ëª…ì€ **ì›ë¬¸ í‘œê¸° ê·¸ëŒ€ë¡œ** ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì •ë³´ê°€ ë¶ˆë¶„ëª…í•˜ë©´ ì„ì˜ë¡œ ì¶”ì •í•˜ì§€ ë§ê³  "ì›ë¬¸ì— ëª…ì‹œ ì—†ìŒ"ì´ë¼ê³  ì ìŠµë‹ˆë‹¤.

[ìŠ¤íƒ€ì¼/ê¸¸ì´]
- í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
- ê° ì„¹ì…˜ ì œëª©ì€ ë°˜ë“œì‹œ Markdown í—¤ë”(###)ë¡œ í‘œê¸°í•˜ê³ , ë³¸ë¬¸ì€ ì¼ë°˜ ë¬¸ë‹¨/ë¶ˆë¦¿ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
- ì „ì²´ ë¶„ëŸ‰ì€ 300~500ë‹¨ì–´ ë‚´ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
- ë¶ˆí•„ìš”í•œ ìˆ˜ì‹/ì¥ì‹ ê¸ˆì§€. í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ.
"""

# 4) ìš”ì•½ User í”„ë¡¬í”„íŠ¸
SUMMARY_USER = """\
[ë¬¸ì„œ ë©”íƒ€]
- ì œëª©: {title}
- ì¶œì²˜: {source}

[ì…ë ¥ ì²­í¬ë“¤]
{chunks}

[ìš”ì²­]
- ìœ„ ê·œì¹™ëŒ€ë¡œ ì„¹ì…˜ ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”.
- ì¤‘ë³µ ë‚´ìš©ì€ ì œê±°í•˜ê³ , ë™ì¼ ê°œë…ì˜ ë‹¤ì–‘í•œ í‘œê¸°ëŠ” ì¼ê´€ë˜ê²Œ í†µì¼í•˜ì„¸ìš”.
"""

# 5) QA ì „ìš© System í”„ë¡¬í”„íŠ¸ (ê·¼ê±°/ë¶ˆí™•ì‹¤ì„± ê°€ë“œ)
QA_SYSTEM = """\
ë‹¹ì‹ ì€ ê¸°ìˆ  ë…¼ë¬¸ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
- ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(ë…¼ë¬¸ ì²­í¬) **ë‚´ì—ì„œë§Œ** ì‚¬ì‹¤ì„ ì¶”ì¶œí•˜ì—¬ ë‹µí•©ë‹ˆë‹¤.
- ì™¸ë¶€ ì§€ì‹ ì¶”ì •/í™˜ê° ê¸ˆì§€. ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ "ì›ë¬¸ì— ëª…ì‹œ ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
- ë‹µë³€ í˜•ì‹:
  1) í•µì‹¬ ë‹µë³€(5~10ë¬¸ì¥, í•œêµ­ì–´)
  2) ê·¼ê±° ì¸ìš© ë¸”ë¡: ê° ë¬¸ì¥ ì•ì— `> `ë¥¼ ë¶™ì—¬ 2~4ì¤„ ì¸ìš©
"""

# 6) QA User í”„ë¡¬í”„íŠ¸
QA_USER = """\
[ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[ìš”ì²­]
- ìœ„ í˜•ì‹ëŒ€ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- ìˆ˜ì¹˜/ëª¨ë¸/ë°ì´í„°ì…‹ì€ ì›ë¬¸ í‘œê¸° ìœ ì§€.
"""

# 7) ì²´ì¸ êµ¬ì„± (ìš”ì•½/QA ë³„ë„)
summary_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SUMMARY_SYSTEM),
        ("user", SUMMARY_USER),
    ]
)
summary_chain = summary_prompt | summary_llm | StrOutputParser()

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", QA_SYSTEM),
        ("user", QA_USER),
    ]
)
qa_chain = qa_prompt | qa_llm | StrOutputParser()


# 8) Utils: ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° + ì¤‘ë³µ ì œê±° (ê°„ë‹¨ ë²„ì „)
def _dedup_lines(text: str) -> str:
    """ì•„ì£¼ ë‹¨ìˆœí•œ ì¤‘ë³µ ì¤„ ì œê±°(ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ ê¸¸ì´ ê°ì†Œì— ìœ ìš©)."""
    seen = set()
    out: List[str] = []
    for line in text.splitlines():
        key = line.strip()
        if key and key not in seen:
            seen.add(key)
            out.append(line)
    return "\n".join(out)


@traceable  # â˜… ì´ 1ì¤„ë§Œ ì¶”ê°€
# 9) Runnable: ìš”ì•½ ì—ì´ì „íŠ¸
def _run_summarize(state):
    """
    state ìš”êµ¬:
    - raw_texts: List[str] ë˜ëŠ” raw_text: str (ë‘˜ ì¤‘ í•˜ë‚˜)
    - meta: dict(title, source) (ì„ íƒ)
    """
    # (1) ì…ë ¥ ìˆ˜ì§‘
    title = (state.get("meta") or {}).get("title", "ì œëª© ë¯¸ìƒ")
    source = (state.get("meta") or {}).get("source", "ì¶œì²˜ ë¯¸ìƒ")

    # raw_texts ìš°ì„ , ì—†ìœ¼ë©´ raw_text ì‚¬ìš©
    raw_texts = state.get("raw_texts")
    if raw_texts is None:
        rt = state.get("raw_text", "")
        raw_texts = [rt] if rt else []

    if not raw_texts:
        return {
            "summary": "ğŸ’¡ ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. raw_text ë˜ëŠ” raw_textsë¥¼ í™•ì¸í•˜ì„¸ìš”."
        }

    # (2) ì²­í¬ í•©ì¹˜ê¸° (ë„ˆë¬´ ê¸¸ë©´ ìƒìœ„ Nê°œë§Œ ì‚¬ìš©, ì—¬ê¸°ì„  ë‹¨ìˆœ ê²°í•©)
    chunks = "\n\n---\n\n".join(raw_texts)
    chunks = _dedup_lines(chunks)

    # (3) ëª¨ë¸ í˜¸ì¶œ
    summary = summary_chain.invoke(
        {
            "title": title,
            "source": source,
            "chunks": chunks,
        }
    )
    return {"summary": summary}


summarizer_agent = RunnableLambda(_run_summarize)


@traceable  # â˜… ì´ 1ì¤„ë§Œ ì¶”ê°€
# 10) Runnable: QA + Retrieval
def qa_with_retrieval(state):
    """
    state ìš”êµ¬:
    - user_input: str (ì§ˆë¬¸)
    - retriever: BaseRetriever-like
    - meta: dict(optional) (ë¬¸ì„œëª…/ì €ì/í˜ì´ì§€ ë“± ë„£ìœ¼ë©´ UX ì¢‹ìŒ)
    - top_k: int(optional)
    """
    question = state.get("user_input", "")
    if not question:
        return {"answer": "ğŸ’¡ ì§ˆë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. user_inputì„ í™•ì¸í•˜ì„¸ìš”."}

    retriever = state.get("retriever")
    if retriever is None:
        return {
            "answer": "ğŸ’¡ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê±°ë‚˜ ê²€ìƒ‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. retrieverê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    top_k = state.get("top_k", 4)
    try:
        docs = retriever.invoke(question)  # í•„ìš” ì‹œ retriever.search_kwargs ì¡°ì •
        docs = docs[:top_k] if len(docs) > top_k else docs
    except Exception as e:
        return {"answer": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"}

    if not docs:
        return {
            "answer": "ğŸ’¡ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ êµ¬ì²´í™”í•˜ê±°ë‚˜ ë¬¸ì„œ ì—…ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”."
        }

    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±: page_content + (ê°€ëŠ¥í•˜ë©´) ë©”íƒ€ì •ë³´ë„ ë§ë¶™ì´ê¸°
    # LangChain ë¬¸ì„œ ê°ì²´ëŠ” .metadataì— source/page ë“± ìˆì„ ìˆ˜ ìˆìŒ
    context_blocks = []
    for i, d in enumerate(docs, 1):
        src = d.metadata.get("source") if hasattr(d, "metadata") else None
        page = d.metadata.get("page") if hasattr(d, "metadata") else None
        header = f"[Doc#{i} | source={src or 'N/A'} | page={page or 'N/A'}]"
        context_blocks.append(f"{header}\n{d.page_content}")

    context = "\n\n------\n\n".join(context_blocks)
    context = _dedup_lines(context)

    answer = qa_chain.invoke({"context": context, "question": question})
    return {"answer": answer}


qa_agent = RunnableLambda(qa_with_retrieval)
